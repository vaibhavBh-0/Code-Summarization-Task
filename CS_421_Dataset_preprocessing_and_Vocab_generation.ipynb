{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G5VHk0H0cISn",
    "outputId": "cfc92c28-b7f0-44e2-8ce2-bd077554557a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 4.9 MB 5.3 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_text -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jr1Hq0GGQrNd"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../python_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_column', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>docstring</th>\n",
       "      <th>code_tokens</th>\n",
       "      <th>docstring_tokens</th>\n",
       "      <th>partition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>def zmq_device(self):\\n        '''\\n        Mu...</td>\n",
       "      <td>Multiprocessing target for the zmq queue device</td>\n",
       "      <td>[def, zmq_device, (, self, ), :, self, ., __se...</td>\n",
       "      <td>[Multiprocessing, target, for, the, zmq, queue...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>def close(self):\\n        '''\\n        Cleanly...</td>\n",
       "      <td>Cleanly shutdown the router socket</td>\n",
       "      <td>[def, close, (, self, ), :, if, self, ., _clos...</td>\n",
       "      <td>[Cleanly, shutdown, the, router, socket]</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>def pre_fork(self, process_manager):\\n        ...</td>\n",
       "      <td>Pre-fork we need to create the zmq router devi...</td>\n",
       "      <td>[def, pre_fork, (, self, ,, process_manager, )...</td>\n",
       "      <td>[Pre, -, fork, we, need, to, create, the, zmq,...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>def _start_zmq_monitor(self):\\n        '''\\n  ...</td>\n",
       "      <td>Starts ZMQ monitor for debugging purposes.\\n  ...</td>\n",
       "      <td>[def, _start_zmq_monitor, (, self, ), :, # Soc...</td>\n",
       "      <td>[Starts, ZMQ, monitor, for, debugging, purpose...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>def post_fork(self, payload_handler, io_loop):...</td>\n",
       "      <td>After forking we need to create all of the loc...</td>\n",
       "      <td>[def, post_fork, (, self, ,, payload_handler, ...</td>\n",
       "      <td>[After, forking, we, need, to, create, all, of...</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                code  \\\n",
       "0  def zmq_device(self):\\n        '''\\n        Mu...   \n",
       "1  def close(self):\\n        '''\\n        Cleanly...   \n",
       "2  def pre_fork(self, process_manager):\\n        ...   \n",
       "3  def _start_zmq_monitor(self):\\n        '''\\n  ...   \n",
       "4  def post_fork(self, payload_handler, io_loop):...   \n",
       "\n",
       "                                           docstring  \\\n",
       "0    Multiprocessing target for the zmq queue device   \n",
       "1                 Cleanly shutdown the router socket   \n",
       "2  Pre-fork we need to create the zmq router devi...   \n",
       "3  Starts ZMQ monitor for debugging purposes.\\n  ...   \n",
       "4  After forking we need to create all of the loc...   \n",
       "\n",
       "                                         code_tokens  \\\n",
       "0  [def, zmq_device, (, self, ), :, self, ., __se...   \n",
       "1  [def, close, (, self, ), :, if, self, ., _clos...   \n",
       "2  [def, pre_fork, (, self, ,, process_manager, )...   \n",
       "3  [def, _start_zmq_monitor, (, self, ), :, # Soc...   \n",
       "4  [def, post_fork, (, self, ,, payload_handler, ...   \n",
       "\n",
       "                                    docstring_tokens partition  \n",
       "0  [Multiprocessing, target, for, the, zmq, queue...     train  \n",
       "1           [Cleanly, shutdown, the, router, socket]     train  \n",
       "2  [Pre, -, fork, we, need, to, create, the, zmq,...     train  \n",
       "3  [Starts, ZMQ, monitor, for, debugging, purpose...     train  \n",
       "4  [After, forking, we, need, to, create, all, of...     train  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAxs6q4mRNHD"
   },
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "import gzip\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Clt7ToufcVtu"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "import tensorflow_text as text\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpujwPIGX-pB"
   },
   "outputs": [],
   "source": [
    "project_id = 'firm-foundation-331820'\n",
    "BUCKET_NAME = 'code-search-python-dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Amgsn317PmU"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_column', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xF8l6IeIFhxY"
   },
   "outputs": [],
   "source": [
    "def extract_file(file_name):\n",
    "  base_file_name = file_name.split('.')[0]\n",
    "\n",
    "  with ZipFile(file_name, mode='r') as zip_obj:\n",
    "    os.makedirs(base_file_name)\n",
    "    os.chdir(base_file_name)\n",
    "\n",
    "    zip_obj.extractall()\n",
    "\n",
    "    os.chdir('../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_YG9H0jFmxk"
   },
   "outputs": [],
   "source": [
    "def download_file(base_url, file_name):\n",
    "  with requests.get(base_url + file_name, stream=False) as r:\n",
    "    with open(file_name, mode='wb') as f:\n",
    "      f.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_CIX468JGaR"
   },
   "source": [
    "Download and extract all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2Zv-qCD5_Eb"
   },
   "outputs": [],
   "source": [
    "base_url = 'https://s3.amazonaws.com/code-search-net/CodeSearchNet/v2/'\n",
    "languages = ['python']\n",
    "extension = '.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Jr3USXAGmeK"
   },
   "outputs": [],
   "source": [
    "for language in languages:\n",
    "  file_name = language + extension\n",
    "  download_file(base_url, file_name=file_name)\n",
    "  extract_file(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lPlGNL_RYHfp"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w31FlOjrlRml",
    "outputId": "33a61c70-e5b3-4466-b8f5-279a9abbe82a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 457461 entries, 0 to 22175\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   code              457461 non-null  object\n",
      " 1   docstring         457461 non-null  object\n",
      " 2   code_tokens       457461 non-null  object\n",
      " 3   docstring_tokens  457461 non-null  object\n",
      " 4   partition         457461 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 20.9+ MB\n"
     ]
    }
   ],
   "source": [
    "language = 'python'\n",
    "base_file_path = f'{language}/{language}/final/jsonl/'\n",
    "columns = ['code', 'docstring', 'code_tokens', 'docstring_tokens', 'partition']\n",
    "\n",
    "df_train_python = pd.concat([pd.read_json((base_file_path + dir + '/' + relative_path), compression='gzip', \n",
    "                                          lines=True)[columns] \n",
    "                             for dir in ['train', 'valid', 'test'] \n",
    "                             for relative_path in os.listdir(base_file_path + dir + '/')])\n",
    "\n",
    "df_train_python.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g74iPxnJUPjh"
   },
   "outputs": [],
   "source": [
    "df_train_python.to_pickle('/content/python_dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqx9kO-FLdqM"
   },
   "source": [
    "Uploading to GCS for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yj23rwtzYVGc",
    "outputId": "83d8ffcf-0752-4d0c-a4d0-f1eec37a43fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project {project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TVdAB0H5Ykx8",
    "outputId": "3d05384f-cf66-40a9-f739-55291dc33e0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://python_dataset.pkl [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "\\\n",
      "Operation completed over 1 objects/1.0 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp python_dataset.pkl gs://{BUCKET_NAME}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "97UNCTehZABI",
    "outputId": "76076fae-bada-4eca-a1b5-b131ce14d4a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://code-search-python-dataset/python_dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "!gsutil ls gs://{BUCKET_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pDztzlIS0vdL"
   },
   "outputs": [],
   "source": [
    "def data_gen_pandas_for_vocab():\n",
    "  for index, row in df_train_python.iterrows():\n",
    "    yield row['code'], row['docstring']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjr5gdK60Hqc"
   },
   "outputs": [],
   "source": [
    "vocab_dataset = Dataset.from_generator(data_gen_pandas_for_vocab, \n",
    "                               output_signature=(tf.TensorSpec(shape=(), dtype=tf.string),\n",
    "                                                 tf.TensorSpec(shape=(), dtype=tf.string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TtAwZYFC1j6y"
   },
   "outputs": [],
   "source": [
    "tokenizer_param = {'lower_case' : True}\n",
    "reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\", \"[SEP]\"]\n",
    "vocab_args = {\n",
    "    'vocab_size' : 33000,\n",
    "    'reserved_tokens' : reserved_tokens,\n",
    "    'bert_tokenizer_params' : tokenizer_param,\n",
    "    'learn_params' : {}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0bTSV1h6nvq"
   },
   "outputs": [],
   "source": [
    "code_ds = vocab_dataset.map(lambda code, _: code).batch(10000).prefetch(5)\n",
    "docstring_ds = vocab_dataset.map(lambda _, docstring: docstring).batch(10000).prefetch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRSROzrxZOLQ"
   },
   "outputs": [],
   "source": [
    "def write_vocab_file(filename, vocab):\n",
    "  with open(filename, mode='w') as f:\n",
    "    for word in vocab:\n",
    "      print(word, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DDd53INLD5a"
   },
   "source": [
    "Creating vocabulary for code and docstring tokenizers using WordPiece Algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsajgPQ6-9QF"
   },
   "outputs": [],
   "source": [
    "code_vocab = bert_vocab.bert_vocab_from_dataset(dataset=code_ds, **vocab_args)\n",
    "write_vocab_file('code_vocab.txt', code_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxc9Ni9haqgn"
   },
   "outputs": [],
   "source": [
    "docstring_vocab = bert_vocab.bert_vocab_from_dataset(dataset=docstring_ds, **vocab_args)\n",
    "write_vocab_file('docstring_vocab.txt', docstring_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UqsaAUhJMP8T"
   },
   "source": [
    "Tokenizers each having their own vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flCqA1e79s9L"
   },
   "outputs": [],
   "source": [
    "code_tokenizer = text.BertTokenizer('code_vocab.txt', **tokenizer_param)\n",
    "docstring_tokenizer = text.BertTokenizer('docstring_vocab.txt', **tokenizer_param)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CS 421 Dataset preprocessing and Vocab generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
